\section{User Evaluation}

\subsection{Introduction}

After completing the implementation of the desktop and web based applications a
thorough user evaluation was carried out of both systems. The user evaluation
functioned as a reliable way to test the usability and likeability of both
system designs. All usability testing performed by the team was done in
accordance with the University of Glasgow ethics procedures. A copy of the
University of Glasgow School of Computer Science Ethics check-list can be found
in Section . . (FILL IN) of the Appendix.

The Evaluation was preformed in a three stage process: participant brief;
think-aloud, and questionnaire. Each of which will be discussed below.

\subsubsection{Participant Brief}

Starting with an introductory briefing, the team member conducting the
evaluation introduced the test participant to the system, provided the user
with a test number and a copy of the user test script, and described the
motivations and aims of the user evaluation that was about to take place.

During this stage the participant was asked to answer a few simple questions to
gauge their competency using desktop and web based applications, and to gauge
their personal interest in the systems domain (sports statistics).

During the introductory brief, it was made clear to the participant that no
personal or identifying information would be collected from them during the
user evaluation. It was decided by the team that this would hopefully decrease
the number of participants who would not complete the evaluation fully, and to
hopefully ease the process of gaining ethical permissions from the University.
Due to the fact that no test participants decided to stop the evaluation half
way through, and gaining ethical approval for the user evaluation was a simple
process, it was felt like this was a beneficial decision.

In accordance with the University's ethical procedures, during the introductory
briefing the test participants were reminded of their right to stop the
evaluation at any time with no requirement to give reason. The participant was
then further reminded that it was not them, but the system that was under
evaluation The participant was also provided with the contact details of the
team member conducting the evaluation, to allow them to contact the team to
answer any questions or after thoughts that they had about the system or the
user evaluation.

A copy of the Participant Brief can be found at the top of the user evaluation
from Appendix~\ref{fig:userEvaluation}.

\subsubsection{Think-Aloud (usability)}

The evaluation was performed using the ”Think-Aloud” technique. The test
participants were encouraged to talk out loud as they performed a series of
tasks, designed to provide a full overview of the complete functionality
provided by both systems.

At this stage the role of the team member conducting the evaluation was to
observe the test candidates interaction with the system, and take note of any
hesitation, possible confusion, or errors encountered when using the system.
The reactions shown by the test participant when interacting with both the web
based and desktop based applications clearly highlighted usability problems
which went unnoticed in the initial system design.

A copy of the Task List for both the Web based and Desktop based applications
can be found in Appendix~\ref{fig:userEvaluation}.

\subsubsection{Questionnaire (likeability)}

The final part of the user evaluation asked the test candidate to complete a
feedback questionnaire. This document asked them to rate their interest in the
applications after their initial experience using the system. At this stage the
test candidates were also given the opportunity to ask any further questions
about the each of the systems. After being thanked for their time and made
aware of the tests completion, every participant was encouraged to get in
contact with a member of the team if they had any further thoughts they wished
to add on the system, after having some time to think about the evaluation
process.

A copy of the Questionnaire can be found in
Appendix~\ref{fig:questionnaireEval}.

\subsubsection{Evaluation Results}

\subsubsection{4.1 Desktop Application}

The user evaluation for the Desktop Application effectively communicated
numerous positive aspects of the design of the Desktop Application, and also
shed light on aspects of the system which needed revising.

The feedback gained from the evaluation was mostly positive, with many test
subjects commenting on the applications pleasant interface and minimal
aesthetic. Numerous test subjects did however leave constructive criticisms on
aspects of the system, mainly concentrating on the applications Print League
and Generate League functionality.

A few points of interest gained from the Desktop evaluation were:

\begin{itemize}
\item Only two of the test subjects partaking in the user evaluation managed to
complete all five test tasks correctly. However, all five test users rated
themselves 'Very Confident' when asked to rate competency using desktop
applications, on a five point Likert scale.
\item Nearly all test users showed hesitation when asked to navigate to ``5
days before the end of the season'' (Task 3). Many commented on their lack of
knowledge of when the end of the season was, even though two tasks previously
they had been asked to navigate to this date and were provided with the date on
the task sheet (Task 1). This displayed a clear lack of communication in the
users mind between the Desktop application they were using, and the information
on the task sheet provided. In addition this highlighted no clear indication of
the range of dates valid for the displayed Baseball season.
\item Even with a small sample size of just five test participants, users
attention span varied wildly. One user gave up on one of the tasks (Task 7 -
Generate a new league, and load this league into the system.) within 20
seconds, after failing to complete the task on their first attempt. Another,
spent over 2 minutes and several attempts when facing problems trying to
complete this part of the user evaluation.
\end{itemize}

The table below discusses the feedback given by the five test participants that
completed the user evaluation. The table provides the feedback gained from the
user, a description of the problem that the user faced when interacting with
the application, and possible future solutions to problems they encountered.

\begin{table}[t]
\begin{tabular}{|l|p{\dimexpr 0.2\linewidth-2\tabcolsep}|p{\dimexpr
0.45\linewidth-2\tabcolsep}|p{\dimexpr 0.35\linewidth-2\tabcolsep}|}
\hline
 & Evaluation Feedback & Description & Design Considerations  \\
\hline
1 & Increase Visibility of League and Division Selection & Users were frequently
selecting the correct League or Division, but not both, presuming that the other
was correct. This made it clear that horizontal radio buttons are not suitable
for displaying which League and Division are currently selected. & Change radio
buttons to a vertical layout to increase League/Division selected visibility.\\
\hline
2 & Increase Visibility of Certificate of Elimination & Majority of test
subjects were unable to find the certificate of elimination information. &
Display this information at the bottom of the user interface, above the first
non trivial elimination details for increased visibility.\\
\hline
3 & Restructure Print League Menu to Aid Usability & Multiple users showed
significant hesitation when interacting with the Print League Menu. Multiple
attempts needed by many to find the correct functionality selection, and
numerous users started a ``just try all of them'' approach. & Provide a ``hover
over for more info'' feature for each option in the print league menu to aid
usability, and provide easily accessible documentation for the print league
functionality.\\
\hline
4 & Adapt heading on Print League ``Name Document'' Window & Multiple users
showed significant hesitation when asked to name their document for printing, as
header on the opening window is headed as "Save" & Change heading on Print out
League Naming Document Window to a more suitable title. Again provide easily
accessible documentation for print league and Generate League functionality.\\
\hline
5 & Provide clear info on file type for Print League and Generate League
Document & Numerous test subjects showed significant confusion when asked to
name the league document they were trying to print, or a new league they were
trying to generate. Many users were unable to complete the task due to being
unsure of appropriate file type. & Provide easily accessible full documentation
for print league and functionality.\\
\hline
6 & Provide ``Next Step'' information for user after generating a new league &
Only a single test subject managed to complete task 7- `Generate a new league
and load the league into the system'. Multiple users presumed that after the
``File Generated'' window was displayed, the task was complete. & Provide ``Next
Step'' information for user in the ``File Generated'' Window. Provide a bullet
point to hint that the generated txt file must now be loaded into the
application from the file menu, or the system should complete this
automatically.\\
\hline
7 & Clearly display start and end of Season dates in user interface & Multiple
users showed significant hesitation when asked to find team information on the
last day of the season. Several resorted to repeatedly clicking next day
continuously until realising the displayed data was not updating. & Display this
information at the bottom of the user interface, potentially above the first non
trivial elimination details for increased visibility.\\
\hline
\end{tabular}
\caption{Desktop Application : User Evaluation Results}
\label{tab:template}
\end{table}

\subsubsection{4.2 Web Application}

Again, the user evaluation of the Web based Application effectively communicated
numerous positive aspects of the design of the Web based Application, and also
shed light on aspects of the system which needed revising.

Unfortunately, the feedback gained from the evaluation of the web application
was not as positive as gathered from the desktop version of the system. Many
test subjects sighted visibility problems, poor navigational options, and
limited features compared with the desktop application as concerns they had with
the system.

The table below discusses the feedback given by the five test participants that
completed the user evaluation. The table provides the feedback gained from the
user, a description of the problems that the user faced when interacting with
the application and future solutions to problems they encountered.

\begin{table}[t]
\begin{tabular}{|l|p{\dimexpr 0.25\linewidth-2\tabcolsep}|p{\dimexpr
0.45\linewidth-2\tabcolsep}|p{\dimexpr 0.3\linewidth-2\tabcolsep}|}
\hline
 & Evaluation Feedback & Description & Design Considerations\\
\hline
1 & Fix display date bug on load of site.& When the site loads up the current
date of the data being displayed is not shown. This information is not displayed
until the user has interacted and clicked on either the `previous day' or `next
day' interface options. & Simple bug fix to display this info when page is first
loaded.\\
\hline
2 & Fix display date bug for non-existent dates. &  One test subject discovered
that the web application was displaying division data for non-existent dates
(i.e displaying data on the 31st September) & Bug Fix to display suitable error
message for non-existent dates.\\
\hline
3 & Clearly highlight URL Manipulation functionality for changing date. &
Majority of test subjects took significant time to realise that the date could
be changed via the web applications URL. Even though all five test users rated
themselves `Very Confident' when asked to rate competency using web based
applications on a five point Likert scale, some participants spent over 90
seconds repeatedly moving through the season on a day by day basis to get to the
date required & A preferable solution would be to include date picker
functionality in the web application.\\
\hline
4 & Clearly display start and end of Season dates in the user interface &
Multiple users showed significant hesitation when asked to find team information
on the last day of the season. Many attempted to find this data in the side bar
of the site, which added to confusion. Several resorted to repeatedly clicking
next day continuously until realising data displayed was not updating. & Display
this information clearly under site side bar to aid user experience.\\
\hline
5 & Keep individual division data visible when moving through dates in the
season. & Some test subjects voiced annoyance at division data being closed from
view when moving through days in the season. Expressed annoyance at having to
repeatedly click open the division they were following to view the divisions
standings. & Simple bug fix to keep relevant division open when moving through
days in the season.\\
\hline
\end{tabular}
\caption{Web Based Application : User Evaluation Results}
\label{tab:template}
\end{table}