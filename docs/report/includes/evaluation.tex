\section{User Evaluation}

\subsection{Introduction}

After completing the implementation of the desktop and web based applications a thorough user evaluation was carried out of both systems. The user evaluation functioned as a reliable way to test the usability and likeability of both system designs. All usability testing preformed by the team was done in accordance with the University of Glasgow ethics procedures. A copy of the University of Glasgows school of Computer Science Ethics checklist can be found in Section . . (FILL IN) of the Appendix. 

The Evaluation was preformed in a three stage process. 


\subsubsection{ Participant Brief }

Starting with an introductory briefing, the team member conducting the evaluation introduced the test participant to the system, provided the user with a test number and a copy of the user test script, and described the motivations and aims of the user evaluation that was about to take place.  

During this stage the participant was asked to answer a few simple questions to gauge their competency using desktop and web based applications, and to gauge their personal interest in the systems domain (sports statistics).

 During the introductory brief, it was made clear to the participant that no personal or identifying information would be collected from them during the user evaluation.  It was decided by the team that this would hopefully decrease the amount of participants who would not complete the evaluation fully, and to hopefully ease the process of gaining eithical permissions from the university . Due to the fact that no test participants decided to stop the evalation half way through, and gaining ethical approval for the user evaluation was a simple process, it was felt like this was a benifical desion . 

In accordance with the universities ethical prodedures, during the introductory breifing the test participant were reminded of their right to stop the evalution at any time with no requirement to give reason. The user was then further reminded that it was not them, but the system that was under evaluation, and the user was provided with the contact details of the team member conducting the evaluation, to allow the user to contact the team to answer any question or after thoughts that they had about the system, or the user evaluation, after it had been completed and the participant had been given some time to think about the two applications under evaluation and the evaluation process.

A copy of the Participant Brief can be found in Section . . .  in the Appendix . (FILL IN !) 

\subsubsection{Think-Aloud (usability) }

The evaluation was performed using the ”Think-Aloud” technique. The test participants were encouraged to talk out loud as they preformed a series of tasks, designed to provide a full overview of the complete functionality provided by both systems. 

At this stage the role of the team member conducting the evaluation was to observe the test candidates interaction with the system, and take note of any hesitation, possible confusion, or errors encountered when using the system. The reactions shown by the test participant when interacting with both the web based and desktop based applications clearly highlighted usability problems which went unnoticed in the teams initial system design. 

A copy of the Task List for botht the Web based and Destop based applications can be found in Section . . .  in the Appendix . (FILL IN !) 

\subsubsection{Questionnaire (likeability)}

The final part of the user evaluation asked the test canddidate to complete a feedback questionare. This document asked them to rate their intest in the applications after their inital experience using the system. At this stage the test candidates were also given the opportunity to ask any further questions about the each of the systems. After being thanked for their time and made aware of the tests completion, every participant was ecouraged to get in contact with a member of the team if they had any further thoughts the wished to add on the system, after having some time to think about the evaluation process.    

A copy of the Questionnaire can be found in Section . . .  in the Appendix . (FILL IN !) 


\subsubsection{Evaluation Results}

\subsubsection{4.1 Desktop Application}

The user evaluation for the Desktop Application effectively communicated numerous positive aspects of the design of the Desktop Application, and also shed light on aspects of the system which needed revising.

 The feedback gained from the evaluation was mostly positive, with many test subjects commenting on the applications plesant interface and minimal aesthetic. Numerous test subjects did however leave constructive critisism's on aspects of the system, mainly concentrating on the applications Print League and Generate League Functionality . 

A few points of interest gained from the Desktop evaluation were : 

\begin{itemize}

\item Only two of the test subjects partaking in the user evaluation managed to complete all five test task correctly. However, all five test users rated themselves 'Very Confident' when asked to rate competency using desktop applications, on a 5 point Likert scale. 

\item Nearly all test users showed hesitation when asked to navigate to '5 day before the end of the season' (Task 3). Many commented on their lack of knowledge of when the end of the season was, even though two task previously they had been asked to navigate to this date and were provided with the date on the task sheet (Task 1). This displayed a clear lack of communication in the users mind between the Desktop application they were using, and the information on the task sheet provied. 
     
\item Even with a small sample size of just five test participants, users attention span varied wildly. One user gave up one of the tasks (Task 7 - Generate a new league , and load this league into the system.) within 20 seconds, after failing to complete the task on their first attempt. Another, spent over 2 minutes and several attempts when facing problems trying to complete this part of the user evaluation.     

\end{itemize}


The table below discusses the feedback given by the 5 test participants that completed the user evaluation. The table provides the feedback gained from the user, a description of the problem that the user faced when interacting with the application, and possible future solutions to problems they encountered. 


\begin{table}[t]

\begin{tabular}{|l|p{\dimexpr 0.25\linewidth-2\tabcolsep}|p{\dimexpr 0.45\linewidth-2\tabcolsep}|p{\dimexpr 0.3\linewidth-2\tabcolsep}|}
\hline
 & Evaluation Feedback & Description & Design Considerations  \\
\hline
1 & Increase Visibility of League and Division Selection & User were frequently selecting the correct League or Division, but not both, presuming that the other was correct. This made it clear that horizontal radio buttons are not suitable for displaying which  League and Division are currently selected   & Change radio buttons to a vertical layout to increase League/Division selected visibility.     \\
\hline
2 & Increase Visibiity of Certifiacte of Elimination & Majority of test subjects were unable to find certifiacte of elimination Information   & Display this information at the bottom of the user interface, above the first non trival elimination details for increased visibility.     \\
\hline
3 & Restructure Print League Menu to Aid Usability & Multiple users showed significant hesitation when intereacting with the Print League Menu. Multiple attempts needed by many to find the correct functionality selection, and numerous users started a "just try all of them" approach. & Provide a "hover over for more info" feature for each option in the print league menu to aid usability, and provide easily accessable full documentation for the print league functionality.   \\
\hline
4 & Adapt Heading on Print League "Name Document" Window &Multiple users showed significant hesitation when asked to name their document for printing, as header on the opening window is headed as "Save"  & Change heading on Print out League Naming Document Window to a more suitable title. Again provide easily accessable full documentation for print league and Generate League functionality    \\
\hline
5 & Provide clear info on file type for Print League and Generate League Document     & Numerous test subjects showed significant confusion when asked to name the league document they were trying to print, or a new league they were trying to generate. Many users were unable to complete the task due to being unsure of appropiate file type.  & Provide easily accessable full documentation for print league and functionality   \\
\hline
6 & Provide "Next Step" Information for user after Generating a New League   & Only a single test subject managed to complete task 7- 'Generate a new league and load the league into the system'. Multiple users presumed that after the "File Generated ! " window was displayed, the task was complete.   & Provide "Next Step" Information for user in the "File Generated " Window. Provide a bullet point to hint that the generated .txt file, must now be loaded into the application from the file menu.     \\
\hline
7 & Clearly Display Start and end of Season dates in UI   & Multiple users showed significant hesitation when asked to find team information on the last day of the season. Several resorted to repeatedly clicking next day continuously until realising data siplayed was not updating. &  Display this information at the bottom of the user interface, potentially above the first non trival elimination details for increased visibility.   \\
\hline

\hline
\end{tabular}
\caption{Desktop Application : User Evaluation Results}
\label{tab:template}
\end{table}
 	 

\subsubsection{4.2 Web Application}


Again, the user evaluation of the Web based Application effectively communicated numerous positive aspects of the design of the Web based Application, and also shed light on aspects of the system which needed revising. 

Unfortunately, the feedback gained from the evaluation of the web application was not as positive as gathered from the desktop version of the system. Many test subjects sighted visibility problems, poor navigational options, and limited features compaired with the desktop application as concerns they had with the system . 

The table below discusses the feedback given by the 5 test participants that completed the user evaluation. The table provides the feedback gained from the user, a description of the problem that the user faced when interacting with the application and future solutions to problems they encountered. 
    



\begin{table}[t]

\begin{tabular}{|l|p{\dimexpr 0.25\linewidth-2\tabcolsep}|p{\dimexpr 0.45\linewidth-2\tabcolsep}|p{\dimexpr 0.3\linewidth-2\tabcolsep}|}
\hline
 & Evaluation Feedback & Description & Design Considerations  \\
\hline
1 & Fix display date bug on load of site.   & When the site loads up the current date of the data being displayed is not shown. This information is not displayed until the user has interacted and clicked on either the 'previous day' or 'next day' interface options    & Simple bug fix to display this info when page is first loaded   \\
\hline

2 & Fix display date bug for nonexistent dates  &  One test subject discovered that the web application was displaying divison data for nonexistent dates (i.e displaying data on the 31st September)  & Bug Fix to display suitable error message for nonexistent dates.   \\
\hline
3 & Clearly highlight Url Manipulation functionality for changing date.   & Majority of test subject took significant time to realise that the date could be changed via the web applications Url. Even though all five test users rated themselves 'Very Confident' when asked to rate competency using desktop applications on a 5 point Likert scale, some participants spent over 90 seconds repeatedly moving through the season on a day by day basis to get to the date required  & Display 'Hint' information below side bar in UI, to make user aware of URl Manipulation option when navigating through dates in season.   \\
\hline

4 &Clearly Display Start and end of Season dates in UI   & Multiple users showed significant hesitation when asked to find team information on the last day of the season. Many attempted to find this data in the side bar of the site, which added to confusion. Several resorted to repeatedly clicking next day continuously until realising data siplayed was not updating.    & Display this information clearly under site side bar to aid user experience.  \\
\hline
5 & Keep Individual division data visible when moving through dates in the season  & Some test subjects voiced annoyance at divsion data being closed from view when moving through days in the season. Expressed annoyance at having to repeatedly click open the division they were following to view the divisions standings    & Simple bug fix to keep relevant division open when moving through days in the season.  \\
\hline
\end{tabular}
\caption{Web Based Application : User Evaluation Results}
\label{tab:template}
\end{table}